# -*- coding:utf-8 -*-
# @Time   :2022/1/27 4:16 ä¸‹åˆ
# @Author :Li Meng qi
# @FileName:rewrite.py
import pycorrector
import torch
from transformers import BertTokenizer, BertForMaskedLM
from collections import defaultdict
import json
from annoy import AnnoyIndex
from query.utils.mle_model import MLEmodel
from query.tokenization import Tokenization
from nltk.util import pad_sequence, ngrams
import os

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))


class RewriteQuery:
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        # è°ƒç”¨tokenizerè¿”å›ä¸€ä¸ªå¥å­çš„input_idsï¼ˆå­—å…¸idï¼‰ã€ token_type_idsï¼ˆç¬¬ä¸€ä¸ªå¥å­ï¼‰ã€ attention_maskï¼ˆmaské®ç½©å‡å°è¯¯å·®ï¼‰ä¿¡æ¯
        self.tokenizer = BertTokenizer.from_pretrained(
            BASE_DIR + "/query/model_data/macbert4csc-base-chinese")
        # åˆå§‹åŒ–æ¨¡å‹å¹¶åŠ è½½å‚æ•°
        self.model = BertForMaskedLM.from_pretrained(
            BASE_DIR + "/query/model_data/macbert4csc-base-chinese")
        # æ¨¡å‹åŠ è½½åˆ°æŒ‡å®šè®¾å¤‡
        self.model = self.model.to(self.device)
        # åŠ è½½ç›¸ä¼¼è¯è¡¨
        self.load_similar_words()
        # åŠ è½½è¯ä¸è¯é¢‘
        self.load_word_freq()
        # åŠ è½½ä¹‹å‰ä¿å­˜çš„ç´¢å¼•æ–‡ä»¶ï¼Œç›®çš„åŠ å¿«æŸ¥è¯¢é€Ÿåº¦
        self.load_index_word_vector()
        # åŠ è½½è¯­è¨€æ¨¡å‹
        self.language_model = MLEmodel().load()

    def load_similar_words(self):
        # è¯»å–ç›¸ä¼¼è¯è¡¨ï¼ˆæ³¨æ„è¯è¡¨çš„ç¼–ç æ ¼å¼æ˜¯GB18030ï¼‰
        with open(BASE_DIR + "/query/model_data/HIT-IRLab.txt", 'r', encoding='GB18030') as f:
            lines = f.readlines()
        # åªç”¨å…¶ä¸­ç›¸ä¼¼çš„è¯
        self.one2other = defaultdict(list)
        for line in lines:
            line_list = line.strip().split()
            if line_list[0].endswith('='):
                word_list = line_list[1:]
                for i, word in enumerate(word_list):
                    self.one2other[word] = word_list  # å…¨éƒ¨è€ƒè™‘
                    # self.one2other[word] = word_list[0:i] + word_list[i + 1:]  # ä¸è€ƒè™‘è‡ªå·±

    def load_word_freq(self):
        with open(BASE_DIR + "/query/model_data/vocab_dic.bin", 'rb') as f_dict:
            w_freq = f_dict.read()
        self.w_freq = json.loads(w_freq)
        temp = defaultdict(int)
        for k, v in self.w_freq.items():
            temp[k] = v
        self.w_freq = temp

    def load_index_word_vector(self):
        with open(BASE_DIR + "/query/model_data/tencent-AILab-ChineseEmbedding/tc_word_index.json", 'r') as fp:
            self.word_index = json.load(fp)
        self.tc_index = AnnoyIndex(200)
        self.tc_index.load(BASE_DIR + "/query/model_data/tencent-AILab-ChineseEmbedding/tc_index_build10.index")
        # åå‘id==>wordæ˜ å°„è¯è¡¨
        self.reverse_word_index = dict([(value, key) for (key, value) in self.word_index.items()])

    def caculate_perplexity(self, tokens, n=2):
        # è®¡ç®—å¥å­çš„å›°æƒ‘åº¦
        # 1ã€è·å–å¥å­çš„ngramè¡¨ç¤ºã€‚
        padded_sent = list(pad_sequence(tokens, pad_left=True, left_pad_symbol="<s>",
                                        pad_right=True, right_pad_symbol="</s>", n=n))
        print(padded_sent)
        print(list(ngrams(padded_sent, n=n)))
        # 2ã€è®¡ç®—å›°æƒ‘åº¦å¹¶è¿”å›ç»“æœ
        return self.language_model.perplexity(list(ngrams(padded_sent, n=n)))

    def query_corrector(self, query):
        """
        è¯­æ³•çº é”™
        """
        with torch.no_grad():
            outputs = self.model(**self.tokenizer(query, padding=True, return_tensors='pt').to(self.device))
        _text = self.tokenizer.decode(torch.argmax(outputs.logits[0], dim=-1), skip_special_tokens=True).replace(' ',
                                                                                                                 '')
        return _text

    def query_unify(self, tokens):
        """
        queryå½’ä¸€
        """
        candidate = []
        for token in tokens:
            other_words = self.one2other[token]
            if len(other_words) == 0:
                candidate.append(token)
            else:
                other_word2weight = []
                for other_word in other_words:
                    other_word2weight.append((other_word, self.w_freq[other_word]))
                max_other_word = sorted(other_word2weight, key=lambda x: x[1], reverse=True)[0][0]
                candidate.append(max_other_word)
        return candidate

    def query_extend(self, query, topk=10):  # è¿˜éœ€è¦ä¼˜åŒ–ï¼ŒåŠ å…¥è¯­è¨€æ¨¡å‹æˆ–è€…beam-searchæ–¹æ³•æ¥æé«˜ç”Ÿæˆå¥å­çš„å‡†ç¡®æ€§
        """
        å¯¹queryè¿›è¡Œæ‰©å±•
        :param query:åˆ†è¯åçš„queryï¼Œç±»å‹åˆ—è¡¨
        :return: è¿”å›æ‰©å±•åçš„ç»“æœ
        """
        # model.logscore("never", "language is".split())  # æ‰“åˆ†
        results = [[] for _ in range(topk)]
        for word in query:
            try:  # OOVé—®é¢˜è¿˜ç”¨åŸæ¥çš„è¯
                # get_nns_by_itemåŸºäºannoyæŸ¥è¯¢è¯æœ€è¿‘çš„10ä¸ªå‘é‡ï¼Œè¿”å›ç»“æœæ˜¯ä¸ªlistï¼Œé‡Œé¢å…ƒç´ æ˜¯ç´¢å¼•
                for i, item in enumerate(self.tc_index.get_nns_by_item(self.word_index[word], topk)):
                    # self.reverse_word_index[item]ç”¨æ¯ä¸ªç´¢å¼•æŸ¥è¯¢word
                    results[i].append(self.reverse_word_index[item])
            except Exception as e:
                for i in range(topk):
                    results[i] += word
        # ä½¿ç”¨è¯­è¨€æ¨¡å‹å¯¹ç”Ÿæˆçš„å¥å­æ‰“åˆ†å¹¶ä¸”æŒ‰ç…§åˆ†æ•°ç”±å°åˆ°å¤§æ’åˆ—ï¼ˆè¿™é‡Œæ˜¯å›°æƒ‘åº¦ï¼Œè¶Šå°è¶Šå¥½ï¼‰
        results = [(''.join(tokens), self.caculate_perplexity(tokens)) for tokens in results]
        results = sorted(results, key=lambda x: x[1])
        return results


if __name__ == '__main__':
    rq = RewriteQuery()
    tokenizer = Tokenization()
    query = 'æ”¯æŒå‘é‡ç§¯æ•™é›ªè§†é¢‘ğŸ˜Š'
    print('åŸæ¥queryï¼š', query)
    query = rq.query_corrector(query)
    print('çº é”™åï¼š', query)
    query_token, _ = tokenizer.hanlp_token_ner(query)
    print('åˆ†è¯åï¼š', query_token)
    result = rq.query_unify(query_token)
    print('å½’ä¸€åï¼š', result)
    result = rq.query_extend(query_token)
    print('æ‹“å±•åï¼š', result)